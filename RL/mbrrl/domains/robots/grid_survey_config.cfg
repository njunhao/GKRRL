# task
domain = grid_survey_mdp
problem = grid_survey_inst_mdp__1

# options
learning = false
learner = lfit
learn_from_failure = false
sync_model = true
noop_actions_allowed = false
noop_no_reward = false
verbose_state = 0
verbose_action = 0
verbose_search = 0
verbose_step = 1
save_file = true

# simulator
simulator_host = 127.0.0.1
simulator_port = 2323

# algorithm options
#exploration_known_threshold = 3
experience = list   		# list or relational

# files
# ppddl_domain_file = domain.ppddl
# ppddl_problem_template_file = grid_survey_inst_mdp_template.pddl
transitions_file = transitions.dat
all_transitions_file = transitions_all.dat
transitions_tmp_file = transitions_tmp.dat
log_conf = ../../scripts/run_experiments/logger.conf
learner_conf = ../../scripts/run_experiments/ruleslearner_logger.conf
rddl_learned_domain_file = learned_domain.rddl
rddl_instance_file = learned_inst.rddl
rddl_learned_file = learned_domain_inst.txt
ppddl_learned_domain_file = learned_domain.ppddl
rules_path = learned_rules.dat

# binaries
rddl_parser = ../../external/prost/simple-rddl-parser
learner_path = ../../external/RulesLearners/bin/ruleslearners
lfit_path = ../../external/RulesLearners/bin/lfit
pasula_path = ../../external/RulesLearners/bin/pasula_learner

# learner
# evaluate_model_req_num_sf =
# evaluate_model_req_num_af =
lfit_learner_max_action_variables = 4
lfit_learner_max_preconditions = 5
lfit_learner_optimal = false
lfit_learner_use_subsumption_tree = true
lfit_learner_conflicts_heuristic_max_iterations = 50
lfit_learner_conflicts_heuristic_max_rules_per_iter = 1000
lfit_learner_score_optimistic_value = 1.0
lfit_learner_score_use_confidence = true
lfit_learner_score_regularization_scaling = 0.02
lfit_learner_score_confidence_interval = 0.1
rddl_write_precondition = true     # write action-preconditions to rddl_domain_path
pasula_alpha_pen = 0.25
pasula_noise_lower_bound = 1e-9
pasula_noise_lower_bound_default_rule = 1e-11
# rule_head_actions_mapping_file = grid_survey_known_mappings.dat

# reward
reward_for_failed_execution = -1
reward_step_forward = false  	# True if domain has immediate rewards which are feedback from simulator 1 step later